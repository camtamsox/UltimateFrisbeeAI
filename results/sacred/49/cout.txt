{'exp_name': 'default', 'display': False, 'restore_fp': None, 'save_rate': 10, 'scenario_name': 'ultimate_frisbee', 'num_episodes': 6000, 'max_episode_len': 10000, 'good_policy': 'matd3', 'adv_policy': 'matd3', 'lr': 0.01, 'gamma': 0.95, 'batch_size': 1024, 'num_layers': 2, 'num_units': 64, 'update_rate': 100, 'critic_zero_if_done': False, 'buff_size': 100000.0, 'tau': 0.01, 'hard_max': False, 'priori_replay': False, 'alpha': 0.6, 'beta': 0.5, 'use_target_action': True, 'policy_update_rate': 1, 'critic_action_noise_stddev': 0.0, 'action_noise_clip': 0.5, 'entropy_coeff': 0.05, 'num_atoms': 51, 'min_val': -400, 'max_val': 0, 'seed': 888340842}
C:\Users\camta\miniconda3\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Using good policy matd3 and adv policy matd3
49
Starting iterations...
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 10088, episodes: 10, mean episode reward: -72266.11111111111, agent episode reward: [-2009.6666666666667, -7541.333333333333, -7560.222222222223, -7555.222222222223, -7555.222222222223, -7576.888888888889, -7534.111111111111, -3594.222222222222, -3530.8888888888887, -3594.222222222222, -3500.3333333333335, -3594.222222222222, -3525.3333333333335, -3594.222222222222], time: 110.935
steps: 10157, episodes: 20, mean episode reward: -163.88888888888889, agent episode reward: [-1007.5555555555555, -1007.5555555555555, -1009.7777777777778, -1007.5555555555555, -1007.5555555555555, -1018.1111111111111, -1007.5555555555555, 969.7777777777778, 1007.5555555555555, 969.7777777777778, 1007.5555555555555, 969.7777777777778, 1007.5555555555555, 969.7777777777778], time: 1.407
steps: 20229, episodes: 30, mean episode reward: -72210.55555555556, agent episode reward: [-2007.5555555555557, -7538.111111111111, -7557.555555555556, -7553.111111111111, -7552.0, -7566.444444444444, -7528.666666666667, -3585.777777777778, -3533.0, -3585.777777777778, -3502.4444444444443, -3585.777777777778, -3528.5555555555557, -3585.777777777778], time: 108.766
steps: 20290, episodes: 40, mean episode reward: -117.22222222222223, agent episode reward: [-1005.6666666666666, -1005.6666666666666, -1005.6666666666666, -1005.6666666666666, -1005.6666666666666, -1009.5555555555555, -1005.6666666666666, 977.3333333333334, 1005.6666666666666, 977.3333333333334, 1005.6666666666666, 977.3333333333334, 1005.6666666666666, 977.3333333333334], time: 1.618
game has been saved from episode: 50
steps: 20379, episodes: 50, mean episode reward: -204.44444444444446, agent episode reward: [-1009.6666666666666, -1009.6666666666666, -1010.7777777777778, -1009.6666666666666, -1009.6666666666666, -1019.6666666666666, -1009.6666666666666, 961.3333333333334, 1009.6666666666666, 961.3333333333334, 1009.6666666666666, 961.3333333333334, 1009.6666666666666, 961.3333333333334], time: 2.101
steps: 30434, episodes: 60, mean episode reward: -72154.44444444444, agent episode reward: [-2005.0, -7538.888888888889, -7553.333333333333, -7550.555555555556, -7550.555555555556, -7555.0, -7529.444444444444, -3575.5555555555557, -3535.5555555555557, -3575.5555555555557, -3505.0, -3575.5555555555557, -3528.8888888888887, -3575.5555555555557], time: 110.061
steps: 40491, episodes: 70, mean episode reward: -76106.66666666667, agent episode reward: [-5936.888888888889, -7539.666666666667, -7554.111111111111, -7551.333333333333, -7551.333333333333, -7560.222222222223, -7530.222222222223, -3578.6666666666665, -3534.777777777778, -3578.6666666666665, -3504.222222222222, -3578.6666666666665, -3529.222222222222, -3578.6666666666665], time: 111.243
