{'exp_name': 'default', 'display': False, 'restore_fp': None, 'save_rate': 10, 'scenario_name': 'ultimate_frisbee', 'num_episodes': 6000, 'max_episode_len': 10000, 'good_policy': 'matd3', 'adv_policy': 'matd3', 'lr': 0.01, 'gamma': 0.95, 'batch_size': 1024, 'num_layers': 2, 'num_units': 64, 'update_rate': 100, 'critic_zero_if_done': False, 'buff_size': 100000.0, 'tau': 0.01, 'hard_max': False, 'priori_replay': False, 'alpha': 0.6, 'beta': 0.5, 'use_target_action': True, 'policy_update_rate': 1, 'critic_action_noise_stddev': 0.0, 'action_noise_clip': 0.5, 'entropy_coeff': 0.05, 'num_atoms': 51, 'min_val': -400, 'max_val': 0, 'seed': 548136951}
C:\Users\camta\miniconda3\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Using good policy matd3 and adv policy matd3
46
Starting iterations...
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 90, episodes: 10, mean episode reward: -167.77777777777777, agent episode reward: [-1012.5555555555555, -1009.7777777777778, -1014.2222222222222, -1009.7777777777778, -1009.7777777777778, -1009.7777777777778, -1012.0, 960.8888888888889, 1009.7777777777778, 1009.7777777777778, 960.8888888888889, 998.1111111111111, 960.8888888888889, 1009.7777777777778], time: 2.967
steps: 183, episodes: 20, mean episode reward: -155.0, agent episode reward: [-1012.8888888888889, -1009.5555555555555, -1010.6666666666666, -1009.5555555555555, -1009.5555555555555, -1009.5555555555555, -1009.5555555555555, 961.7777777777778, 1009.5555555555555, 1009.5555555555555, 961.7777777777778, 1002.3333333333334, 961.7777777777778, 1009.5555555555555], time: 1.781
steps: 330, episodes: 30, mean episode reward: -247.22222222222223, agent episode reward: [-1028.4444444444443, -1013.4444444444445, -1028.4444444444443, -1013.4444444444445, -1013.4444444444445, -1013.4444444444445, -1016.7777777777778, 946.2222222222222, 1013.4444444444445, 1013.4444444444445, 946.2222222222222, 1001.2222222222222, 946.2222222222222, 1013.4444444444445], time: 2.689
steps: 404, episodes: 40, mean episode reward: -143.33333333333334, agent episode reward: [-1014.5555555555555, -1007.8888888888889, -1014.0, -1007.8888888888889, -1007.8888888888889, -1007.8888888888889, -1010.1111111111111, 968.4444444444445, 1007.8888888888889, 1007.8888888888889, 968.4444444444445, 997.8888888888889, 968.4444444444445, 1007.8888888888889], time: 1.785
C:\Users\camta\Desktop\Coding\UltimateFrisbeeAI\UltimateFrisbeeAI\tf2marl\multiagent\environment.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  np.save(f, np.array(self.game_history))
game has been saved from episode: 50
steps: 529, episodes: 50, mean episode reward: -210.55555555555554, agent episode reward: [-1017.7777777777778, -1012.2222222222222, -1018.3333333333334, -1012.2222222222222, -1012.2222222222222, -1012.2222222222222, -1013.3333333333334, 951.1111111111111, 1012.2222222222222, 1012.2222222222222, 951.1111111111111, 997.7777777777778, 951.1111111111111, 1012.2222222222222], time: 2.418
steps: 612, episodes: 60, mean episode reward: -138.33333333333334, agent episode reward: [-1012.0, -1008.1111111111111, -1010.3333333333334, -1008.1111111111111, -1008.1111111111111, -1008.1111111111111, -1008.1111111111111, 967.5555555555555, 1008.1111111111111, 1008.1111111111111, 967.5555555555555, 997.5555555555555, 967.5555555555555, 1008.1111111111111], time: 1.769
steps: 757, episodes: 70, mean episode reward: -226.11111111111111, agent episode reward: [-1015.6666666666666, -1014.0, -1014.5555555555555, -1014.0, -1014.0, -1014.0, -1014.0, 944.0, 1014.0, 1014.0, 944.0, 1000.1111111111111, 944.0, 1014.0], time: 2.627
steps: 806, episodes: 80, mean episode reward: -74.44444444444444, agent episode reward: [-1004.7777777777778, -1004.7777777777778, -1004.7777777777778, -1004.7777777777778, -1004.7777777777778, -1004.7777777777778, -1004.7777777777778, 980.8888888888889, 1004.7777777777778, 1004.7777777777778, 980.8888888888889, 1002.0, 980.8888888888889, 1004.7777777777778], time: 1.572
steps: 890, episodes: 90, mean episode reward: -120.55555555555556, agent episode reward: [-1008.1111111111111, -1007.5555555555555, -1010.8888888888889, -1007.5555555555555, -1007.5555555555555, -1007.5555555555555, -1007.5555555555555, 969.7777777777778, 1007.5555555555555, 1007.5555555555555, 969.7777777777778, 1004.2222222222222, 969.7777777777778, 1007.5555555555555], time: 1.904
game has been saved from episode: 100
steps: 999, episodes: 100, mean episode reward: -197.22222222222223, agent episode reward: [-1021.0, -1011.5555555555555, -1018.2222222222222, -1011.5555555555555, -1012.1111111111111, -1011.5555555555555, -1016.0, 953.7777777777778, 1011.5555555555555, 1011.5555555555555, 953.7777777777778, 1008.7777777777778, 953.7777777777778, 1011.5555555555555], time: 2.317
steps: 1103, episodes: 110, mean episode reward: -180.0, agent episode reward: [-1013.4444444444445, -1010.6666666666666, -1012.3333333333334, -1010.6666666666666, -1010.6666666666666, -1010.6666666666666, -1010.6666666666666, 957.3333333333334, 1010.6666666666666, 1002.8888888888889, 957.3333333333334, 1002.8888888888889, 957.3333333333334, 1010.6666666666666], time: 2.106
steps: 1188, episodes: 120, mean episode reward: -143.88888888888889, agent episode reward: [-1008.8888888888889, -1008.8888888888889, -1008.8888888888889, -1008.8888888888889, -1008.8888888888889, -1008.8888888888889, -1008.8888888888889, 964.4444444444445, 1008.8888888888889, 1008.8888888888889, 964.4444444444445, 998.3333333333334, 964.4444444444445, 1008.8888888888889], time: 1.962
steps: 1310, episodes: 130, mean episode reward: -235.55555555555554, agent episode reward: [-1028.111111111111, -1012.5555555555555, -1025.888888888889, -1012.5555555555555, -1013.6666666666666, -1012.5555555555555, -1018.1111111111111, 949.7777777777778, 1012.5555555555555, 1012.5555555555555, 949.7777777777778, 1000.8888888888889, 949.7777777777778, 1012.5555555555555], time: 2.362
steps: 1391, episodes: 140, mean episode reward: -131.11111111111111, agent episode reward: [-1008.5555555555555, -1008.5555555555555, -1010.2222222222222, -1008.5555555555555, -1008.5555555555555, -1008.5555555555555, -1008.5555555555555, 965.7777777777778, 1008.5555555555555, 1008.5555555555555, 965.7777777777778, 1007.4444444444445, 965.7777777777778, 1008.5555555555555], time: 1.965
game has been saved from episode: 150
steps: 1482, episodes: 150, mean episode reward: -153.33333333333334, agent episode reward: [-1014.1111111111111, -1009.1111111111111, -1014.1111111111111, -1009.1111111111111, -1009.6666666666666, -1009.1111111111111, -1009.1111111111111, 963.5555555555555, 1009.1111111111111, 1009.1111111111111, 963.5555555555555, 1003.0, 963.5555555555555, 1009.1111111111111], time: 1.92
steps: 1581, episodes: 160, mean episode reward: -156.11111111111111, agent episode reward: [-1013.3333333333334, -1009.4444444444445, -1009.4444444444445, -1009.4444444444445, -1009.4444444444445, -1009.4444444444445, -1009.4444444444445, 962.2222222222222, 1009.4444444444445, 1008.8888888888889, 962.2222222222222, 999.4444444444445, 962.2222222222222, 1009.4444444444445], time: 2.125
steps: 1674, episodes: 170, mean episode reward: -185.55555555555554, agent episode reward: [-1017.0, -1009.7777777777778, -1019.7777777777778, -1009.7777777777778, -1009.7777777777778, -1009.7777777777778, -1013.1111111111111, 960.8888888888889, 1009.7777777777778, 1009.7777777777778, 960.8888888888889, 991.4444444444445, 960.8888888888889, 1009.7777777777778], time: 2.408
steps: 1768, episodes: 180, mean episode reward: -167.22222222222223, agent episode reward: [-1013.5555555555555, -1009.6666666666666, -1014.1111111111111, -1009.6666666666666, -1010.2222222222222, -1009.6666666666666, -1012.4444444444445, 961.3333333333334, 1009.6666666666666, 1009.6666666666666, 961.3333333333334, 999.1111111111111, 961.3333333333334, 1009.6666666666666], time: 2.008
steps: 1894, episodes: 190, mean episode reward: -223.88888888888889, agent episode reward: [-1021.4444444444445, -1012.5555555555555, -1017.0, -1012.5555555555555, -1013.6666666666666, -1012.5555555555555, -1017.0, 949.7777777777778, 1012.5555555555555, 1012.5555555555555, 949.7777777777778, 995.8888888888889, 949.7777777777778, 1012.5555555555555], time: 2.424
game has been saved from episode: 200
steps: 1979, episodes: 200, mean episode reward: -147.77777777777777, agent episode reward: [-1012.4444444444445, -1009.1111111111111, -1010.2222222222222, -1009.1111111111111, -1009.1111111111111, -1009.1111111111111, -1009.1111111111111, 963.5555555555555, 1009.1111111111111, 1008.0, 963.5555555555555, 1003.5555555555555, 963.5555555555555, 1009.1111111111111], time: 2.125
steps: 2037, episodes: 210, mean episode reward: -93.33333333333333, agent episode reward: [-1008.6666666666666, -1005.8888888888889, -1005.8888888888889, -1005.8888888888889, -1005.8888888888889, -1005.8888888888889, -1007.5555555555555, 976.4444444444445, 1005.8888888888889, 1005.8888888888889, 976.4444444444445, 1005.3333333333334, 976.4444444444445, 1005.8888888888889], time: 1.773
steps: 2157, episodes: 220, mean episode reward: -228.33333333333334, agent episode reward: [-1020.6666666666666, -1012.3333333333334, -1018.4444444444445, -1012.3333333333334, -1012.3333333333334, -1012.3333333333334, -1012.8888888888889, 950.6666666666666, 1012.3333333333334, 1009.0, 950.6666666666666, 987.3333333333334, 950.6666666666666, 1012.3333333333334], time: 2.273
steps: 2243, episodes: 230, mean episode reward: -154.44444444444446, agent episode reward: [-1011.5555555555555, -1008.7777777777778, -1008.7777777777778, -1008.7777777777778, -1009.8888888888889, -1008.7777777777778, -1008.7777777777778, 964.8888888888889, 1008.7777777777778, 1008.7777777777778, 964.8888888888889, 989.8888888888889, 964.8888888888889, 1008.7777777777778], time: 1.973
steps: 2330, episodes: 240, mean episode reward: -155.0, agent episode reward: [-1012.1111111111111, -1009.3333333333334, -1011.5555555555555, -1009.3333333333334, -1009.3333333333334, -1009.3333333333334, -1009.3333333333334, 962.6666666666666, 1009.3333333333334, 1009.3333333333334, 962.6666666666666, 999.3333333333334, 962.6666666666666, 1009.3333333333334], time: 1.874
game has been saved from episode: 250
steps: 2445, episodes: 250, mean episode reward: -207.22222222222223, agent episode reward: [-1020.8888888888889, -1012.5555555555555, -1013.6666666666666, -1012.5555555555555, -1012.5555555555555, -1012.5555555555555, -1012.5555555555555, 949.7777777777778, 1012.5555555555555, 1012.5555555555555, 949.7777777777778, 1003.1111111111111, 949.7777777777778, 1012.5555555555555], time: 2.239
steps: 2538, episodes: 260, mean episode reward: -160.0, agent episode reward: [-1012.7777777777778, -1009.4444444444445, -1016.1111111111111, -1009.4444444444445, -1010.0, -1009.4444444444445, -1010.5555555555555, 962.2222222222222, 1009.4444444444445, 1009.4444444444445, 962.2222222222222, 1002.7777777777778, 962.2222222222222, 1009.4444444444445], time: 2.004
steps: 12603, episodes: 270, mean episode reward: -77205.55555555556, agent episode reward: [-7512.111111111111, -7384.333333333333, -7544.888888888889, -7515.444444444444, -7552.111111111111, -7524.888888888889, -7552.111111111111, -3581.777777777778, -3528.4444444444443, -3331.777777777778, -3581.777777777778, -3547.8888888888887, -3581.777777777778, -3466.222222222222], time: 110.371
