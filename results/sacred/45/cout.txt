{'exp_name': 'default', 'display': False, 'restore_fp': None, 'save_rate': 10, 'scenario_name': 'ultimate_frisbee', 'num_episodes': 6000, 'max_episode_len': 10000, 'good_policy': 'matd3', 'adv_policy': 'matd3', 'lr': 0.01, 'gamma': 0.95, 'batch_size': 1024, 'num_layers': 2, 'num_units': 64, 'update_rate': 100, 'critic_zero_if_done': False, 'buff_size': 100000.0, 'tau': 0.01, 'hard_max': False, 'priori_replay': False, 'alpha': 0.6, 'beta': 0.5, 'use_target_action': True, 'policy_update_rate': 1, 'critic_action_noise_stddev': 0.0, 'action_noise_clip': 0.5, 'entropy_coeff': 0.05, 'num_atoms': 51, 'min_val': -400, 'max_val': 0, 'seed': 385205015}
C:\Users\camta\miniconda3\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Using good policy matd3 and adv policy matd3
45
Starting iterations...
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 80002, episodes: 10, mean episode reward: -534260.0, agent episode reward: [-46818.666666666664, -46853.666666666664, -37418.666666666664, -46811.444444444445, -46835.333333333336, -46850.333333333336, -46846.444444444445, -30875.777777777777, -30855.777777777777, -30890.222222222223, -30813.0, -30628.0, -30889.666666666668, -30873.0], time: 902.424
steps: 130012, episodes: 20, mean episode reward: -304977.77777777775, agent episode reward: [-27182.222222222223, -27203.333333333332, -21471.11111111111, -27179.444444444445, -27191.666666666668, -27202.222222222223, -27198.333333333332, -17217.222222222223, -17201.666666666668, -17226.666666666668, -17178.88888888889, -17087.222222222223, -17225.555555555555, -17212.222222222223], time: 568.216
steps: 200020, episodes: 30, mean episode reward: -526925.5555555555, agent episode reward: [-46796.666666666664, -46853.333333333336, -30152.777777777777, -46814.444444444445, -46831.11111111111, -46851.11111111111, -46847.77777777778, -30881.11111111111, -30744.444444444445, -30891.11111111111, -30816.666666666668, -30681.11111111111, -30891.11111111111, -30872.777777777777], time: 799.096
steps: 300020, episodes: 40, mean episode reward: -688425.5555555555, agent episode reward: [-59908.333333333336, -59955.0, -49351.666666666664, -59902.77777777778, -59928.333333333336, -59950.555555555555, -59948.333333333336, -39980.555555555555, -39963.88888888889, -40000.0, -39908.333333333336, -39648.333333333336, -39999.444444444445, -39980.0], time: 1124.796
C:\Users\camta\Desktop\Coding\UltimateFrisbeeAI\UltimateFrisbeeAI\tf2marl\multiagent\environment.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  np.save(f, np.array(self.game_history))
game has been saved from episode: 50
steps: 360030, episodes: 50, mean episode reward: -454505.55555555556, agent episode reward: [-40273.666666666664, -40303.11111111111, -28799.777777777777, -40267.0, -40287.555555555555, -40300.88888888889, -40295.88888888889, -26323.0, -26303.0, -26336.88888888889, -26268.555555555555, -26090.777777777777, -26336.333333333332, -26319.11111111111], time: 658.397
steps: 430036, episodes: 60, mean episode reward: -458246.1111111111, agent episode reward: [-40274.0, -40303.444444444445, -32504.0, -40267.88888888889, -40289.555555555555, -40300.666666666664, -40296.77777777778, -26322.11111111111, -26305.444444444445, -26336.0, -26269.333333333332, -26123.222222222223, -26334.333333333332, -26319.333333333332], time: 784.786
steps: 490044, episodes: 70, mean episode reward: -379602.77777777775, agent episode reward: [-33728.11111111111, -33752.555555555555, -24998.11111111111, -33724.22222222222, -33739.77777777778, -33748.11111111111, -33745.88888888889, -21768.0, -21746.333333333332, -21781.333333333332, -21723.555555555555, -21599.666666666668, -21780.777777777777, -21766.333333333332], time: 696.51
