{'exp_name': 'default', 'display': False, 'restore_fp': None, 'save_rate': 10, 'scenario_name': 'ultimate_frisbee', 'num_episodes': 6000, 'max_episode_len': 10000, 'good_policy': 'matd3', 'adv_policy': 'matd3', 'lr': 0.01, 'gamma': 0.95, 'batch_size': 1024, 'num_layers': 2, 'num_units': 64, 'update_rate': 100, 'critic_zero_if_done': False, 'buff_size': 100000.0, 'tau': 0.01, 'hard_max': False, 'priori_replay': False, 'alpha': 0.6, 'beta': 0.5, 'use_target_action': True, 'policy_update_rate': 1, 'critic_action_noise_stddev': 0.0, 'action_noise_clip': 0.5, 'entropy_coeff': 0.05, 'num_atoms': 51, 'min_val': -400, 'max_val': 0, 'seed': 897367903}
C:\Users\camta\miniconda3\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Using good policy matd3 and adv policy matd3
58
episode: 1
Starting iterations...
episode: 2
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
episode: 3
episode: 4
turnover
episode: 5
episode: 6
episode: 7
episode: 8
episode: 9
episode: 10
episode: 11
steps: 57, episodes: 10, mean episode reward: -56.666666666666664, agent episode reward: [-1008.1111111111111, -1002.5555555555555, -1002.5555555555555, -1002.5555555555555, -1002.5555555555555, -1002.5555555555555, -1002.5555555555555, 989.7777777777778, 1002.5555555555555, 989.7777777777778, 1002.5555555555555, 989.7777777777778, 1002.5555555555555, 989.7777777777778], time: 2.033
episode: 12
episode: 13
episode: 14
episode: 15
episode: 16
episode: 17
episode: 18
episode: 19
episode: 20
episode: 21
steps: 67, episodes: 20, mean episode reward: -18.88888888888889, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 997.1111111111111], time: 0.793
episode: 22
episode: 23
episode: 24
episode: 25
turnover
episode: 26
turnover
episode: 27
episode: 28
episode: 29
episode: 30
episode: 31
steps: 77, episodes: 30, mean episode reward: -19.444444444444443, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.5555555555555], time: 1.154
episode: 32
episode: 33
turnover
episode: 34
episode: 35
episode: 36
episode: 37
episode: 38
episode: 39
episode: 40
turnover
episode: 41
steps: 87, episodes: 40, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.147
episode: 42
episode: 43
episode: 44
episode: 45
episode: 46
episode: 47
episode: 48
episode: 49
episode: 50
game has been saved from episode: 50
episode: 51
steps: 97, episodes: 50, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.173
episode: 52
episode: 53
episode: 54
episode: 55
episode: 56
episode: 57
episode: 58
episode: 59
episode: 60
episode: 61
steps: 107, episodes: 60, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.185
episode: 62
episode: 63
episode: 64
episode: 65
turnover
episode: 66
episode: 67
episode: 68
episode: 69
episode: 70
episode: 71
steps: 117, episodes: 70, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.126
episode: 72
episode: 73
episode: 74
episode: 75
episode: 76
episode: 77
episode: 78
episode: 79
turnover
episode: 80
episode: 81
steps: 127, episodes: 80, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.096
episode: 82
episode: 83
episode: 84
episode: 85
episode: 86
episode: 87
episode: 88
episode: 89
episode: 90
turnover
episode: 91
steps: 137, episodes: 90, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.079
episode: 92
episode: 93
episode: 94
episode: 95
episode: 96
episode: 97
episode: 98
episode: 99
episode: 100
game has been saved from episode: 100
episode: 101
steps: 147, episodes: 100, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.058
episode: 102
episode: 103
episode: 104
episode: 105
episode: 106
episode: 107
episode: 108
turnover
episode: 109
episode: 110
episode: 111
steps: 157, episodes: 110, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.084
episode: 112
episode: 113
episode: 114
episode: 115
episode: 116
turnover
episode: 117
episode: 118
turnover
episode: 119
turnover
episode: 120
episode: 121
steps: 167, episodes: 120, mean episode reward: -20.0, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.0], time: 1.188
episode: 122
turnover
episode: 123
episode: 124
episode: 125
episode: 126
episode: 127
episode: 128
episode: 129
episode: 130
episode: 131
steps: 177, episodes: 130, mean episode reward: -19.444444444444443, agent episode reward: [-1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, -1001.0, 996.0, 1001.0, 996.0, 1001.0, 996.5555555555555, 1001.0, 996.0], time: 1.155
