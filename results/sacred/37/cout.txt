{'exp_name': 'default', 'display': False, 'restore_fp': None, 'save_rate': 10, 'scenario_name': 'ultimate_frisbee', 'num_episodes': 6000, 'max_episode_len': 10000, 'good_policy': 'matd3', 'adv_policy': 'matd3', 'lr': 0.01, 'gamma': 0.95, 'batch_size': 1024, 'num_layers': 2, 'num_units': 64, 'update_rate': 100, 'critic_zero_if_done': False, 'buff_size': 100000.0, 'tau': 0.01, 'hard_max': False, 'priori_replay': False, 'alpha': 0.6, 'beta': 0.5, 'use_target_action': True, 'policy_update_rate': 1, 'critic_action_noise_stddev': 0.0, 'action_noise_clip': 0.5, 'entropy_coeff': 0.05, 'num_atoms': 51, 'min_val': -400, 'max_val': 0, 'seed': 270043439}
C:\Users\camta\miniconda3\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Using good policy matd3 and adv policy matd3
37
Starting iterations...
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 50011, episodes: 10, mean episode reward: -332057.22222222225, agent episode reward: [-33774.444444444445, -33767.77777777778, -33759.444444444445, -33728.88888888889, -33728.88888888889, -6001.111111111111, -33728.88888888889, -17543.333333333332, -17409.444444444445, -17566.666666666668, -17701.666666666668, -17782.222222222223, -17782.222222222223, -17782.222222222223], time: 541.73
steps: 90023, episodes: 20, mean episode reward: -174046.11111111112, agent episode reward: [-20665.777777777777, -20661.333333333332, -20656.333333333332, -20638.0, -20638.0, -4001.3333333333335, -20638.0, -6523.666666666667, -6482.0, -6505.333333333333, -6620.333333333333, -6672.0, -6672.0, -6672.0], time: 436.003
steps: 150032, episodes: 30, mean episode reward: -332065.0, agent episode reward: [-33774.333333333336, -33767.666666666664, -33758.77777777778, -33728.77777777778, -33728.77777777778, -6001.0, -33728.77777777778, -17543.444444444445, -17549.0, -17437.333333333332, -17701.777777777777, -17781.777777777777, -17781.777777777777, -17781.777777777777], time: 656.391
steps: 200044, episodes: 40, mean episode reward: -332047.22222222225, agent episode reward: [-33755.11111111111, -33768.444444444445, -33759.555555555555, -33729.0, -33729.0, -6001.222222222223, -33729.0, -17543.777777777777, -17456.0, -17527.11111111111, -17701.0, -17782.666666666668, -17782.666666666668, -17782.666666666668], time: 545.182
game has been saved from episode: 50
steps: 290046, episodes: 50, mean episode reward: -569080.0, agent episode reward: [-53435.77777777778, -53426.88888888889, -53412.444444444445, -53364.666666666664, -53364.666666666664, -9000.222222222223, -53364.666666666664, -34070.333333333336, -34032.0, -33948.11111111111, -34324.22222222222, -34445.333333333336, -34445.333333333336, -34445.333333333336], time: 981.858
steps: 350054, episodes: 60, mean episode reward: -411074.44444444444, agent episode reward: [-40327.77777777778, -40320.555555555555, -40310.0, -40273.88888888889, -40273.88888888889, -7000.555555555556, -40273.88888888889, -23056.666666666668, -23059.444444444445, -22928.333333333332, -23242.777777777777, -23335.555555555555, -23335.555555555555, -23335.555555555555], time: 654.892
steps: 410059, episodes: 70, mean episode reward: -332078.3333333333, agent episode reward: [-33772.77777777778, -33767.22222222222, -33758.333333333336, -33728.333333333336, -33728.333333333336, -6000.555555555556, -33728.333333333336, -17565.0, -17548.333333333332, -17438.88888888889, -17702.222222222223, -17780.0, -17780.0, -17780.0], time: 653.965
steps: 470073, episodes: 80, mean episode reward: -411104.44444444444, agent episode reward: [-40330.555555555555, -40321.666666666664, -40310.0, -40274.444444444445, -40274.444444444445, -7001.111111111111, -40274.444444444445, -23077.777777777777, -22966.666666666668, -23017.777777777777, -23242.222222222223, -23337.777777777777, -23337.777777777777, -23337.777777777777], time: 654.658
steps: 540079, episodes: 90, mean episode reward: -490066.1111111111, agent episode reward: [-46882.22222222222, -46873.88888888889, -46860.555555555555, -46819.444444444445, -46819.444444444445, -8000.555555555556, -46819.444444444445, -28561.11111111111, -28477.222222222223, -28495.555555555555, -28783.333333333332, -28891.11111111111, -28891.11111111111, -28891.11111111111], time: 764.178
game has been saved from episode: 100
steps: 600089, episodes: 100, mean episode reward: -331997.77777777775, agent episode reward: [-33712.77777777778, -33767.77777777778, -33759.444444444445, -33728.88888888889, -33728.88888888889, -6001.111111111111, -33728.88888888889, -17543.333333333332, -17547.777777777777, -17430.555555555555, -17701.666666666668, -17782.222222222223, -17782.222222222223, -17782.222222222223], time: 656.002
steps: 680095, episodes: 110, mean episode reward: -569103.3333333334, agent episode reward: [-53435.77777777778, -53426.88888888889, -53411.88888888889, -53364.666666666664, -53364.666666666664, -9000.222222222223, -53364.666666666664, -34098.666666666664, -33940.88888888889, -34034.77777777778, -34324.22222222222, -34445.333333333336, -34445.333333333336, -34445.333333333336], time: 867.994
steps: 770096, episodes: 120, mean episode reward: -569071.6666666666, agent episode reward: [-53435.666666666664, -53426.77777777778, -53412.88888888889, -53364.555555555555, -53364.555555555555, -9000.111111111111, -53364.555555555555, -34068.77777777778, -33941.0, -34033.77777777778, -34324.333333333336, -34444.88888888889, -34444.88888888889, -34444.88888888889], time: 975.127
