{'exp_name': 'default', 'display': False, 'restore_fp': None, 'save_rate': 10, 'scenario_name': 'ultimate_frisbee', 'num_episodes': 6000, 'max_episode_len': 10000, 'good_policy': 'matd3', 'adv_policy': 'matd3', 'lr': 0.01, 'gamma': 0.95, 'batch_size': 1024, 'num_layers': 2, 'num_units': 64, 'update_rate': 100, 'critic_zero_if_done': False, 'buff_size': 100000.0, 'tau': 0.01, 'hard_max': False, 'priori_replay': False, 'alpha': 0.6, 'beta': 0.5, 'use_target_action': True, 'policy_update_rate': 1, 'critic_action_noise_stddev': 0.0, 'action_noise_clip': 0.5, 'entropy_coeff': 0.05, 'num_atoms': 51, 'min_val': -400, 'max_val': 0, 'seed': 210565664}
C:\Users\camta\miniconda3\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
Using good policy matd3 and adv policy matd3
47
Starting iterations...
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
C:\Users\camta\miniconda3\lib\site-packages\numpy\core\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
steps: 60012, episodes: 10, mean episode reward: -465437.22222222225, agent episode reward: [-40321.11111111111, -40303.88888888889, -40291.11111111111, -40015.0, -40289.444444444445, -40302.77777777778, -40317.77777777778, -26337.777777777777, -26230.0, -26193.88888888889, -26272.222222222223, -26165.555555555555, -26337.777777777777, -26058.88888888889], time: 677.385
steps: 120022, episodes: 20, mean episode reward: -465453.3333333333, agent episode reward: [-40320.555555555555, -40303.333333333336, -40289.444444444445, -40031.11111111111, -40290.0, -40302.77777777778, -40316.11111111111, -26335.555555555555, -26236.11111111111, -26194.444444444445, -26272.777777777777, -26166.11111111111, -26335.555555555555, -26059.444444444445], time: 652.922
steps: 150044, episodes: 30, mean episode reward: -155169.44444444444, agent episode reward: [-14111.333333333334, -14103.0, -14099.111111111111, -14005.222222222223, -14098.555555555555, -14103.555555555555, -14109.111111111111, -8120.888888888889, -8076.444444444444, -8062.0, -8088.666666666667, -8053.111111111111, -8120.888888888889, -8017.555555555556], time: 335.267
steps: 200066, episodes: 40, mean episode reward: -310358.3333333333, agent episode reward: [-27217.444444444445, -27204.666666666668, -27194.666666666668, -27053.0, -27195.222222222223, -27204.11111111111, -27214.666666666668, -17232.0, -17155.88888888889, -17128.666666666668, -17179.777777777777, -17108.666666666668, -17232.0, -17037.555555555555], time: 558.859
game has been saved from episode: 50
steps: 260079, episodes: 50, mean episode reward: -465456.6666666667, agent episode reward: [-40320.88888888889, -40303.666666666664, -40290.88888888889, -40029.22222222222, -40289.77777777778, -40303.666666666664, -40318.11111111111, -26336.88888888889, -26235.222222222223, -26194.11111111111, -26272.444444444445, -26165.777777777777, -26336.88888888889, -26059.11111111111], time: 647.909
steps: 310101, episodes: 60, mean episode reward: -310308.8888888889, agent episode reward: [-27218.0, -27204.666666666668, -27194.666666666668, -27005.777777777777, -27195.222222222223, -27204.11111111111, -27215.222222222223, -17232.0, -17152.555555555555, -17128.666666666668, -17179.777777777777, -17108.666666666668, -17232.0, -17037.555555555555], time: 551.729
steps: 360130, episodes: 70, mean episode reward: -387902.77777777775, agent episode reward: [-33774.88888888889, -33754.88888888889, -33743.22222222222, -33520.444444444445, -33744.333333333336, -33754.88888888889, -33770.444444444445, -21788.444444444445, -21694.0, -21660.11111111111, -21725.11111111111, -21636.222222222223, -21788.444444444445, -21547.333333333332], time: 514.797
steps: 420145, episodes: 80, mean episode reward: -387881.6666666667, agent episode reward: [-33771.11111111111, -33753.88888888889, -33743.333333333336, -33520.0, -33742.22222222222, -33753.88888888889, -33766.666666666664, -21784.444444444445, -21689.444444444445, -21660.555555555555, -21726.11111111111, -21637.222222222223, -21784.444444444445, -21548.333333333332], time: 598.068
steps: 490152, episodes: 90, mean episode reward: -465429.44444444444, agent episode reward: [-40320.77777777778, -40303.555555555555, -40290.22222222222, -40008.555555555555, -40289.666666666664, -40303.0, -40316.88888888889, -26336.444444444445, -26232.555555555555, -26193.666666666668, -26272.555555555555, -26165.88888888889, -26336.444444444445, -26059.222222222223], time: 763.317
game has been saved from episode: 100
steps: 550165, episodes: 100, mean episode reward: -465441.1111111111, agent episode reward: [-40321.77777777778, -40304.0, -40290.666666666664, -40020.11111111111, -40290.11111111111, -40303.444444444445, -40317.88888888889, -26338.222222222223, -26228.222222222223, -26192.11111111111, -26272.11111111111, -26165.444444444445, -26338.222222222223, -26058.777777777777], time: 668.089
steps: 630172, episodes: 110, mean episode reward: -620540.0, agent episode reward: [-53427.0, -53404.22222222222, -53385.333333333336, -52988.666666666664, -53386.444444444445, -53401.444444444445, -53422.555555555555, -35445.77777777778, -35308.555555555555, -35258.555555555555, -35364.11111111111, -35221.88888888889, -35445.77777777778, -35079.666666666664], time: 887.997
